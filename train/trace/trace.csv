Kernel_id,categary,name,Occupancy %
0,cpu_op,aten::arange,0
1,cpu_op,aten::empty,0
2,cpu_op,aten::arange,0
3,cpu_op,aten::resize_,0
4,cpu_op,aten::empty,0
5,cpu_op,aten::eq,0
6,cpu_op,aten::any,0
7,cpu_op,aten::as_strided,0
8,cpu_op,aten::is_nonzero,0
9,cpu_op,aten::item,0
10,cpu_op,aten::_local_scalar_dense,0
11,cpu_op,aten::arange,0
12,cpu_op,aten::empty,0
13,cpu_op,aten::arange,0
14,cpu_op,aten::resize_,0
15,cpu_op,aten::min,0
16,cpu_op,aten::as_strided,0
17,cpu_op,aten::as_strided,0
18,cpu_op,aten::item,0
19,cpu_op,aten::_local_scalar_dense,0
20,cpu_op,aten::empty,0
21,cpu_op,aten::to,0
22,cpu_op,aten::_to_copy,0
23,cpu_op,aten::empty_strided,0
24,cpu_op,aten::copy_,0
25,cpu_op,aten::sum,0
26,cpu_op,aten::as_strided,0
27,cpu_op,aten::to,0
28,cpu_op,aten::_to_copy,0
29,cpu_op,aten::empty_strided,0
30,cpu_op,aten::copy_,0
31,cpu_op,aten::item,0
32,cpu_op,aten::_local_scalar_dense,0
33,cpu_op,aten::empty,0
34,cpu_op,aten::to,0
35,cpu_op,aten::_to_copy,0
36,cpu_op,aten::empty_strided,0
37,cpu_op,aten::copy_,0
38,cpu_op,aten::clamp,0
39,cpu_op,aten::pow,0
40,cpu_op,aten::result_type,0
41,cpu_op,aten::to,0
42,cpu_op,aten::reshape,0
43,cpu_op,aten::_reshape_alias,0
44,cpu_op,aten::mul,0
45,cpu_op,aten::matmul,0
46,cpu_op,aten::mm,0
47,cpu_op,GSpMM,0
48,cpu_op,aten::zeros,0
49,cpu_op,aten::empty,0
50,cpu_op,aten::zero_,0
51,cpu_op,aten::fill_,0
52,cpu_op,aten::arange,0
53,cpu_op,aten::empty,0
54,cpu_op,aten::arange,0
55,cpu_op,aten::resize_,0
56,cpu_op,aten::empty,0
57,cpu_op,aten::to,0
58,cpu_op,aten::_to_copy,0
59,cpu_op,aten::empty_strided,0
60,cpu_op,aten::copy_,0
61,cpu_op,aten::clamp,0
62,cpu_op,aten::pow,0
63,cpu_op,aten::result_type,0
64,cpu_op,aten::to,0
65,cpu_op,aten::reshape,0
66,cpu_op,aten::_reshape_alias,0
67,cpu_op,aten::mul,0
68,cpu_op,aten::add,0
69,cpu_op,aten::relu,0
70,cpu_op,aten::clamp_min,0
71,cpu_op,aten::empty,0
72,cpu_op,aten::clamp_min,0
73,cpu_op,aten::resize_,0
74,cpu_op,aten::dropout,0
75,cpu_op,aten::arange,0
76,cpu_op,aten::empty,0
77,cpu_op,aten::arange,0
78,cpu_op,aten::resize_,0
79,cpu_op,aten::empty,0
80,cpu_op,aten::eq,0
81,cpu_op,aten::any,0
82,cpu_op,aten::as_strided,0
83,cpu_op,aten::is_nonzero,0
84,cpu_op,aten::item,0
85,cpu_op,aten::_local_scalar_dense,0
86,cpu_op,aten::arange,0
87,cpu_op,aten::empty,0
88,cpu_op,aten::arange,0
89,cpu_op,aten::resize_,0
90,cpu_op,aten::min,0
91,cpu_op,aten::as_strided,0
92,cpu_op,aten::as_strided,0
93,cpu_op,aten::item,0
94,cpu_op,aten::_local_scalar_dense,0
95,cpu_op,aten::empty,0
96,cpu_op,aten::to,0
97,cpu_op,aten::_to_copy,0
98,cpu_op,aten::empty_strided,0
99,cpu_op,aten::copy_,0
100,cpu_op,aten::sum,0
101,cpu_op,aten::as_strided,0
102,cpu_op,aten::to,0
103,cpu_op,aten::_to_copy,0
104,cpu_op,aten::empty_strided,0
105,cpu_op,aten::copy_,0
106,cpu_op,aten::item,0
107,cpu_op,aten::_local_scalar_dense,0
108,cpu_op,aten::empty,0
109,cpu_op,aten::to,0
110,cpu_op,aten::_to_copy,0
111,cpu_op,aten::empty_strided,0
112,cpu_op,aten::copy_,0
113,cpu_op,aten::clamp,0
114,cpu_op,aten::pow,0
115,cpu_op,aten::result_type,0
116,cpu_op,aten::to,0
117,cpu_op,aten::reshape,0
118,cpu_op,aten::_reshape_alias,0
119,cpu_op,aten::mul,0
120,cpu_op,aten::matmul,0
121,cpu_op,aten::mm,0
122,cpu_op,GSpMM,0
123,cpu_op,aten::zeros,0
124,cpu_op,aten::empty,0
125,cpu_op,aten::zero_,0
126,cpu_op,aten::fill_,0
127,cpu_op,aten::arange,0
128,cpu_op,aten::empty,0
129,cpu_op,aten::arange,0
130,cpu_op,aten::resize_,0
131,cpu_op,aten::empty,0
132,cpu_op,aten::to,0
133,cpu_op,aten::_to_copy,0
134,cpu_op,aten::empty_strided,0
135,cpu_op,aten::copy_,0
136,cpu_op,aten::clamp,0
137,cpu_op,aten::pow,0
138,cpu_op,aten::result_type,0
139,cpu_op,aten::to,0
140,cpu_op,aten::reshape,0
141,cpu_op,aten::_reshape_alias,0
142,cpu_op,aten::mul,0
143,cpu_op,aten::add,0
144,Kernel,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}, function_traits<{lambda(long)#1}>::result_type*)",2
145,async_gpu,cudaLaunchKernel,0
146,Runtime,cudaLaunchKernel,0
147,async_gpu,cudaLaunchKernel,0
148,Kernel,"void dgl::aten::impl::_CSRGetRowNNZKernel<int>(int const*, int const*, int*, long)",2
149,async_gpu,cudaLaunchKernel,0
150,Runtime,cudaLaunchKernel,0
151,async_gpu,cudaLaunchKernel,0
152,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<int, int, bool, at::native::(anonymous namespace)::CompareEqFunctor<int> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<int, int, bool, at::native::(anonymous namespace)::CompareEqFunctor<int> >, at::detail::Array<char*, 2>)",1
153,async_gpu,cudaLaunchKernel,0
154,Runtime,cudaLaunchKernel,0
155,async_gpu,cudaLaunchKernel,0
156,Kernel,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::or_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#24}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4> >(at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::or_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#24}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4>)",0
157,async_gpu,cudaLaunchKernel,0
158,Runtime,cudaLaunchKernel,0
159,async_gpu,cudaLaunchKernel,0
160,Memcpy,Memcpy DtoH (Device -> Pageable),0
161,async_gpu,cudaMemcpyAsync,0
162,Runtime,cudaMemcpyAsync,0
163,async_gpu,cudaMemcpyAsync,0
164,Runtime,cudaStreamSynchronize,0
165,Kernel,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}, function_traits<{lambda(long)#1}>::result_type*)",2
166,async_gpu,cudaLaunchKernel,0
167,Runtime,cudaLaunchKernel,0
168,async_gpu,cudaLaunchKernel,0
169,Kernel,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<int, at::native::MinOps<int>, unsigned int, int, 4> >(at::native::ReduceOp<int, at::native::MinOps<int>, unsigned int, int, 4>)",0
170,async_gpu,cudaLaunchKernel,0
171,Runtime,cudaLaunchKernel,0
172,async_gpu,cudaLaunchKernel,0
173,Memcpy,Memcpy DtoH (Device -> Pageable),0
174,async_gpu,cudaMemcpyAsync,0
175,Runtime,cudaMemcpyAsync,0
176,async_gpu,cudaMemcpyAsync,0
177,Runtime,cudaStreamSynchronize,0
178,Kernel,"void dgl::aten::impl::_BinaryElewiseKernel<int, dgl::aten::arith::LT>(int const*, int, int*, long)",2
179,async_gpu,cudaLaunchKernel,0
180,Runtime,cudaLaunchKernel,0
181,async_gpu,cudaLaunchKernel,0
182,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#22}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#22}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
183,async_gpu,cudaLaunchKernel,0
184,Runtime,cudaLaunchKernel,0
185,async_gpu,cudaLaunchKernel,0
186,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#12}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#12}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
187,async_gpu,cudaLaunchKernel,0
188,Runtime,cudaLaunchKernel,0
189,async_gpu,cudaLaunchKernel,0
190,Kernel,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)",0
191,async_gpu,cudaLaunchKernel,0
192,Runtime,cudaLaunchKernel,0
193,async_gpu,cudaLaunchKernel,0
194,Memcpy,Memcpy DtoH (Device -> Pageable),0
195,async_gpu,cudaMemcpyAsync,0
196,Runtime,cudaMemcpyAsync,0
197,async_gpu,cudaMemcpyAsync,0
198,Runtime,cudaStreamSynchronize,0
199,Kernel,"void dgl::aten::impl::_CSRGetRowNNZKernel<int>(int const*, int const*, int*, long)",2
200,async_gpu,cudaLaunchKernel,0
201,Runtime,cudaLaunchKernel,0
202,async_gpu,cudaLaunchKernel,0
203,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
204,async_gpu,cudaLaunchKernel,0
205,Runtime,cudaLaunchKernel,0
206,async_gpu,cudaLaunchKernel,0
207,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
208,async_gpu,cudaLaunchKernel,0
209,Runtime,cudaLaunchKernel,0
210,async_gpu,cudaLaunchKernel,0
211,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
212,async_gpu,cudaLaunchKernel,0
213,Runtime,cudaLaunchKernel,0
214,async_gpu,cudaLaunchKernel,0
215,Kernel,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1})",100
216,async_gpu,cudaLaunchKernel,0
217,Runtime,cudaLaunchKernel,0
218,async_gpu,cudaLaunchKernel,0
219,Runtime,cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,0
220,Runtime,cudaStreamGetCaptureInfo,0
221,Runtime,cudaEventQuery,0
222,Runtime,cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,0
223,Runtime,cudaFuncSetAttribute,0
224,Runtime,cudaFuncSetAttribute,0
225,Kernel,void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x128_32x3_nn_align1>(cutlass_80_tensorop_s1688gemm_64x128_32x3_nn_align1::Params),0
226,async_gpu,cudaLaunchKernel,0
227,Runtime,cudaLaunchKernel,0
228,async_gpu,cudaLaunchKernel,0
229,Kernel,"void splitKreduce_kernel<float, float, float, float, true, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, void*, long, float*, int*)",69
230,async_gpu,cudaLaunchKernel,0
231,Runtime,cudaLaunchKernel,0
232,async_gpu,cudaLaunchKernel,0
233,Runtime,cudaStreamGetCaptureInfo,0
234,Runtime,cudaStreamGetCaptureInfo,0
235,Runtime,cudaStreamGetCaptureInfo,0
236,Runtime,cudaEventRecord,0
237,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",9
238,async_gpu,cudaLaunchKernel,0
239,Runtime,cudaLaunchKernel,0
240,async_gpu,cudaLaunchKernel,0
241,Kernel,"void dgl::cuda::_FillKernel<float>(float*, unsigned long, float)",11
242,async_gpu,cudaLaunchKernel,0
243,Runtime,cudaLaunchKernel,0
244,async_gpu,cudaLaunchKernel,0
245,Kernel,"void cusparse::matrix_scalar_multiply_kernel<cusparse::MatrixWiseMulPolicy, true, long, float, float>(long, long, long, cusparse::KernelCoeff<float>, float*)",69
246,async_gpu,cudaLaunchKernel,0
247,Runtime,cudaLaunchKernel,0
248,async_gpu,cudaLaunchKernel,0
249,Kernel,"void cusparse::partition_kernel<128, int, int>(int const*, int, int, int, int, int*)",1
250,async_gpu,cudaLaunchKernel,0
251,Runtime,cudaLaunchKernel,0
252,async_gpu,cudaLaunchKernel,0
253,Kernel,"void cusparse::csrmm_v2_kernel<cusparse::CsrMMPolicy<int, float, float, float>, false, false, false, int, int, int, int, float, float, float>(int, int, int, int, int, int, cusparse::KernelCoeffs<float>, int, int, int const*, int const*, int const*, float const*, long, float const*, int, long, float*, int, long)",14
254,async_gpu,cudaLaunchKernel,0
255,Runtime,cudaLaunchKernel,0
256,async_gpu,cudaLaunchKernel,0
257,Kernel,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}, function_traits<{lambda(long)#1}>::result_type*)",2
258,async_gpu,cudaLaunchKernel,0
259,Runtime,cudaLaunchKernel,0
260,async_gpu,cudaLaunchKernel,0
261,Kernel,"void dgl::aten::impl::_CSRGetRowNNZKernel<int>(int const*, int const*, int*, long)",2
262,async_gpu,cudaLaunchKernel,0
263,Runtime,cudaLaunchKernel,0
264,async_gpu,cudaLaunchKernel,0
265,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
266,async_gpu,cudaLaunchKernel,0
267,Runtime,cudaLaunchKernel,0
268,async_gpu,cudaLaunchKernel,0
269,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
270,async_gpu,cudaLaunchKernel,0
271,Runtime,cudaLaunchKernel,0
272,async_gpu,cudaLaunchKernel,0
273,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
274,async_gpu,cudaLaunchKernel,0
275,Runtime,cudaLaunchKernel,0
276,async_gpu,cudaLaunchKernel,0
277,Kernel,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1})",17
278,async_gpu,cudaLaunchKernel,0
279,Runtime,cudaLaunchKernel,0
280,async_gpu,cudaLaunchKernel,0
281,Kernel,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > const&)::{lambda(int)#1})",17
282,async_gpu,cudaLaunchKernel,0
283,Runtime,cudaLaunchKernel,0
284,async_gpu,cudaLaunchKernel,0
285,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",9
286,async_gpu,cudaLaunchKernel,0
287,Runtime,cudaLaunchKernel,0
288,async_gpu,cudaLaunchKernel,0
289,Kernel,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}, function_traits<{lambda(long)#1}>::result_type*)",2
290,async_gpu,cudaLaunchKernel,0
291,Runtime,cudaLaunchKernel,0
292,async_gpu,cudaLaunchKernel,0
293,Kernel,"void dgl::aten::impl::_CSRGetRowNNZKernel<int>(int const*, int const*, int*, long)",2
294,async_gpu,cudaLaunchKernel,0
295,Runtime,cudaLaunchKernel,0
296,async_gpu,cudaLaunchKernel,0
297,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<int, int, bool, at::native::(anonymous namespace)::CompareEqFunctor<int> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<int, int, bool, at::native::(anonymous namespace)::CompareEqFunctor<int> >, at::detail::Array<char*, 2>)",1
298,async_gpu,cudaLaunchKernel,0
299,Runtime,cudaLaunchKernel,0
300,async_gpu,cudaLaunchKernel,0
301,Kernel,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::or_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#24}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4> >(at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::or_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#24}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4>)",0
302,async_gpu,cudaLaunchKernel,0
303,Runtime,cudaLaunchKernel,0
304,async_gpu,cudaLaunchKernel,0
305,Memcpy,Memcpy DtoH (Device -> Pageable),0
306,async_gpu,cudaMemcpyAsync,0
307,Runtime,cudaMemcpyAsync,0
308,async_gpu,cudaMemcpyAsync,0
309,Runtime,cudaStreamSynchronize,0
310,Kernel,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}, function_traits<{lambda(long)#1}>::result_type*)",2
311,async_gpu,cudaLaunchKernel,0
312,Runtime,cudaLaunchKernel,0
313,async_gpu,cudaLaunchKernel,0
314,Kernel,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<int, at::native::MinOps<int>, unsigned int, int, 4> >(at::native::ReduceOp<int, at::native::MinOps<int>, unsigned int, int, 4>)",0
315,async_gpu,cudaLaunchKernel,0
316,Runtime,cudaLaunchKernel,0
317,async_gpu,cudaLaunchKernel,0
318,Memcpy,Memcpy DtoH (Device -> Pageable),0
319,async_gpu,cudaMemcpyAsync,0
320,Runtime,cudaMemcpyAsync,0
321,async_gpu,cudaMemcpyAsync,0
322,Runtime,cudaStreamSynchronize,0
323,Kernel,"void dgl::aten::impl::_BinaryElewiseKernel<int, dgl::aten::arith::LT>(int const*, int, int*, long)",2
324,async_gpu,cudaLaunchKernel,0
325,Runtime,cudaLaunchKernel,0
326,async_gpu,cudaLaunchKernel,0
327,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#22}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#22}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
328,async_gpu,cudaLaunchKernel,0
329,Runtime,cudaLaunchKernel,0
330,async_gpu,cudaLaunchKernel,0
331,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#12}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#12}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
332,async_gpu,cudaLaunchKernel,0
333,Runtime,cudaLaunchKernel,0
334,async_gpu,cudaLaunchKernel,0
335,Kernel,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)",0
336,async_gpu,cudaLaunchKernel,0
337,Runtime,cudaLaunchKernel,0
338,async_gpu,cudaLaunchKernel,0
339,Memcpy,Memcpy DtoH (Device -> Pageable),0
340,async_gpu,cudaMemcpyAsync,0
341,Runtime,cudaMemcpyAsync,0
342,async_gpu,cudaMemcpyAsync,0
343,Runtime,cudaStreamSynchronize,0
344,Kernel,"void dgl::aten::impl::_CSRGetRowNNZKernel<int>(int const*, int const*, int*, long)",2
345,async_gpu,cudaLaunchKernel,0
346,Runtime,cudaLaunchKernel,0
347,async_gpu,cudaLaunchKernel,0
348,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
349,async_gpu,cudaLaunchKernel,0
350,Runtime,cudaLaunchKernel,0
351,async_gpu,cudaLaunchKernel,0
352,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
353,async_gpu,cudaLaunchKernel,0
354,Runtime,cudaLaunchKernel,0
355,async_gpu,cudaLaunchKernel,0
356,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
357,async_gpu,cudaLaunchKernel,0
358,Runtime,cudaLaunchKernel,0
359,async_gpu,cudaLaunchKernel,0
360,Kernel,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1})",17
361,async_gpu,cudaLaunchKernel,0
362,Runtime,cudaLaunchKernel,0
363,async_gpu,cudaLaunchKernel,0
364,Runtime,cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,0
365,Runtime,cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,0
366,Runtime,cudaFuncSetAttribute,0
367,Runtime,cudaFuncSetAttribute,0
368,Kernel,void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_16x6_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_16x6_nn_align1::Params),4
369,async_gpu,cudaLaunchKernel,0
370,Runtime,cudaLaunchKernel,0
371,async_gpu,cudaLaunchKernel,0
372,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",4
373,async_gpu,cudaLaunchKernel,0
374,Runtime,cudaLaunchKernel,0
375,async_gpu,cudaLaunchKernel,0
376,Kernel,"void dgl::cuda::_FillKernel<float>(float*, unsigned long, float)",11
377,async_gpu,cudaLaunchKernel,0
378,Runtime,cudaLaunchKernel,0
379,async_gpu,cudaLaunchKernel,0
380,Kernel,"void cusparse::matrix_scalar_multiply_kernel<cusparse::MatrixWiseMulPolicy, false, long, float, float>(long, long, long, cusparse::KernelCoeff<float>, float*)",69
381,async_gpu,cudaLaunchKernel,0
382,Runtime,cudaLaunchKernel,0
383,async_gpu,cudaLaunchKernel,0
384,Kernel,"void cusparse::partition_kernel<128, int, int>(int const*, int, int, int, int, int*)",1
385,async_gpu,cudaLaunchKernel,0
386,Runtime,cudaLaunchKernel,0
387,async_gpu,cudaLaunchKernel,0
388,Kernel,"void cusparse::csrmm_v2_kernel<cusparse::CsrMMPolicy<int, float, float, float>, false, false, false, int, int, int, int, float, float, float>(int, int, int, int, int, int, cusparse::KernelCoeffs<float>, int, int, int const*, int const*, int const*, float const*, long, float const*, int, long, float*, int, long)",14
389,async_gpu,cudaLaunchKernel,0
390,Runtime,cudaLaunchKernel,0
391,async_gpu,cudaLaunchKernel,0
392,Kernel,"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#10}::operator()() const::{lambda(long)#1}, function_traits<{lambda(long)#1}>::result_type*)",2
393,async_gpu,cudaLaunchKernel,0
394,Runtime,cudaLaunchKernel,0
395,async_gpu,cudaLaunchKernel,0
396,Kernel,"void dgl::aten::impl::_CSRGetRowNNZKernel<int>(int const*, int const*, int*, long)",2
397,async_gpu,cudaLaunchKernel,0
398,Runtime,cudaLaunchKernel,0
399,async_gpu,cudaLaunchKernel,0
400,Kernel,"void at::native::unrolled_elementwise_kernel<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)",1
401,async_gpu,cudaLaunchKernel,0
402,Runtime,cudaLaunchKernel,0
403,async_gpu,cudaLaunchKernel,0
404,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
405,async_gpu,cudaLaunchKernel,0
406,Runtime,cudaLaunchKernel,0
407,async_gpu,cudaLaunchKernel,0
408,Kernel,"void at::native::vectorized_elementwise_kernel<4, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",1
409,async_gpu,cudaLaunchKernel,0
410,Runtime,cudaLaunchKernel,0
411,async_gpu,cudaLaunchKernel,0
412,Kernel,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::MulFunctor<float> > const&)::{lambda(int)#1})",8
413,async_gpu,cudaLaunchKernel,0
414,Runtime,cudaLaunchKernel,0
415,async_gpu,cudaLaunchKernel,0
416,Kernel,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > const&)::{lambda(int)#1})",8
417,async_gpu,cudaLaunchKernel,0
418,Runtime,cudaLaunchKernel,0
419,async_gpu,cudaLaunchKernel,0
420,Runtime,cudaDeviceSynchronize,0
421,None,process_name,0
422,None,process_labels,0
423,None,process_sort_index,0
424,None,process_name,0
425,None,process_labels,0
426,None,process_sort_index,0
427,None,process_name,0
428,None,process_labels,0
429,None,process_sort_index,0
430,None,process_name,0
431,None,process_labels,0
432,None,process_sort_index,0
433,None,process_name,0
434,None,process_labels,0
435,None,process_sort_index,0
436,None,process_name,0
437,None,process_labels,0
438,None,process_sort_index,0
439,None,process_name,0
440,None,process_labels,0
441,None,process_sort_index,0
442,None,process_name,0
443,None,process_labels,0
444,None,process_sort_index,0
445,None,process_name,0
446,None,process_labels,0
447,None,process_sort_index,0
448,None,thread_name,0
449,None,thread_sort_index,0
450,None,thread_name,0
451,None,thread_sort_index,0
452,Trace,PyTorch Profiler (0),0
453,None,process_sort_index,0
454,None,Iteration Start: PyTorch Profiler,0
455,None,Record Window End,0
